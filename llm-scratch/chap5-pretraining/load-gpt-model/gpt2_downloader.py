#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPT-2 æ¨¡å‹ä¸‹è½½å™¨
è°ƒç”¨ gpt_download.py ä¸­çš„å‡½æ•°æ¥ä¸‹è½½ä¸åŒå¤§å°çš„ GPT-2 æ¨¡å‹
"""

import os
import sys
import time
from gpt_download import download_and_load_gpt2


class GPT2Downloader:
    """GPT-2 æ¨¡å‹ä¸‹è½½å™¨ç±»"""
    
    def __init__(self):
        self.models_dir = "gpt2"
        self.supported_models = {
            "124M": {
                "name": "GPT-2 Small",
                "params": "117M parameters",
                "layers": 12,
                "hidden_size": 768,
                "heads": 12,
                "size_estimate": "~500MB"
            },
            "355M": {
                "name": "GPT-2 Medium", 
                "params": "354M parameters",
                "layers": 24,
                "hidden_size": 1024,
                "heads": 16,
                "size_estimate": "~1.4GB"
            },
            "774M": {
                "name": "GPT-2 Large",
                "params": "774M parameters", 
                "layers": 36,
                "hidden_size": 1280,
                "heads": 20,
                "size_estimate": "~3.1GB"
            },
            "1558M": {
                "name": "GPT-2 XL",
                "params": "1.5B parameters",
                "layers": 48, 
                "hidden_size": 1600,
                "heads": 25,
                "size_estimate": "~6.2GB"
            }
        }
    
    def show_welcome(self):
        """æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯"""
        print("=" * 60)
        print("ğŸ¤– GPT-2 æ¨¡å‹ä¸‹è½½å™¨")
        print("=" * 60)
        print("ğŸ“¥ æ”¯æŒä¸‹è½½æ‰€æœ‰ OpenAI GPT-2 æ¨¡å‹å¤§å°")
        print("ğŸ”„ è‡ªåŠ¨æ–­ç‚¹ç»­ä¼ ï¼Œæ”¯æŒæ‰¹é‡ä¸‹è½½")
        print("=" * 60)
    
    def show_available_models(self):
        """æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹"""
        print("\nğŸ“‹ å¯ç”¨çš„ GPT-2 æ¨¡å‹:")
        print("-" * 70)
        print(f"{'å¤§å°':<8} {'åç§°':<15} {'å‚æ•°':<18} {'å±‚æ•°':<6} {'å¤§å°ä¼°è®¡':<12}")
        print("-" * 70)
        
        for model_id, info in self.supported_models.items():
            print(f"{model_id:<8} {info['name']:<15} {info['params']:<18} "
                  f"{info['layers']:<6} {info['size_estimate']:<12}")
        
        print("-" * 70)
        total_size = sum([float(info['size_estimate'].replace('~', '').replace('GB', '').replace('MB', '')) 
                         for info in self.supported_models.values()])
        print(f"ğŸ’¾ æ‰€æœ‰æ¨¡å‹æ€»å¤§å°çº¦: ~11.0GB")
    
    def check_model_exists(self, model_size):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»å­˜åœ¨"""
        model_path = os.path.join(self.models_dir, model_size)
        if not os.path.exists(model_path):
            return False
        
        # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = [
            "model.ckpt.data-00000-of-00001",
            "model.ckpt.index", 
            "model.ckpt.meta",
            "hparams.json",
            "encoder.json",
            "vocab.bpe"
        ]
        
        for file in required_files:
            if not os.path.exists(os.path.join(model_path, file)):
                return False
        
        return True
    
    def get_model_disk_size(self, model_size):
        """è·å–æ¨¡å‹åœ¨ç£ç›˜ä¸Šçš„å®é™…å¤§å°ï¼ˆMBï¼‰"""
        model_path = os.path.join(self.models_dir, model_size)
        if not os.path.exists(model_path):
            return 0
        
        total_size = 0
        for root, dirs, files in os.walk(model_path):
            for file in files:
                file_path = os.path.join(root, file)
                if os.path.exists(file_path):
                    total_size += os.path.getsize(file_path)
        
        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    
    def download_single_model(self, model_size, force_redownload=False):
        """ä¸‹è½½å•ä¸ªæ¨¡å‹"""
        if model_size not in self.supported_models:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹å¤§å°: {model_size}")
            print(f"âœ… æ”¯æŒçš„å¤§å°: {', '.join(self.supported_models.keys())}")
            return False
        
        model_info = self.supported_models[model_size]
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not force_redownload and self.check_model_exists(model_size):
            actual_size = self.get_model_disk_size(model_size)
            print(f"âœ… {model_info['name']} ({model_size}) å·²å­˜åœ¨")
            print(f"ğŸ“ ç£ç›˜å¤§å°: {actual_size:.1f} MB")
            
            while True:
                choice = input("æ˜¯å¦é‡æ–°ä¸‹è½½? (y/n): ").lower().strip()
                if choice in ['n', 'no']:
                    return True
                elif choice in ['y', 'yes']:
                    break
                else:
                    print("è¯·è¾“å…¥ y æˆ– n")
        
        print(f"\nğŸš€ å¼€å§‹ä¸‹è½½ {model_info['name']} ({model_size})")
        print(f"ğŸ“Š {model_info['params']} | {model_info['layers']} å±‚ | é¢„è®¡å¤§å°: {model_info['size_estimate']}")
        print("-" * 50)
        
        try:
            start_time = time.time()
            
            # è°ƒç”¨ gpt_download.py ä¸­çš„å‡½æ•°
            settings, params = download_and_load_gpt2(
                model_size=model_size,
                models_dir=self.models_dir
            )
            
            end_time = time.time()
            download_time = end_time - start_time
            
            if settings is not None and params is not None:
                actual_size = self.get_model_disk_size(model_size)
                
                print(f"\nâœ… {model_info['name']} ä¸‹è½½å®Œæˆ!")
                print(f"â±ï¸  ä¸‹è½½æ—¶é—´: {download_time:.1f} ç§’")
                print(f"ğŸ’¾ å®é™…å¤§å°: {actual_size:.1f} MB")
                
                # éªŒè¯æ¨¡å‹é…ç½®
                print(f"\nğŸ“‹ æ¨¡å‹é…ç½®éªŒè¯:")
                expected = model_info
                print(f"   å±‚æ•°: {settings.get('n_layer', 'N/A')} (é¢„æœŸ: {expected['layers']})")
                print(f"   éšè—å±‚: {settings.get('n_embd', 'N/A')} (é¢„æœŸ: {expected['hidden_size']})")
                print(f"   æ³¨æ„åŠ›å¤´: {settings.get('n_head', 'N/A')} (é¢„æœŸ: {expected['heads']})")
                print(f"   è¯æ±‡è¡¨: {settings.get('n_vocab', 'N/A')}")
                print(f"   ä¸Šä¸‹æ–‡: {settings.get('n_ctx', 'N/A')}")
                
                if 'wte' in params:
                    print(f"   TokenåµŒå…¥: {params['wte'].shape}")
                
                return True
            else:
                print(f"âŒ {model_info['name']} ä¸‹è½½å¤±è´¥")
                return False
                
        except KeyboardInterrupt:
            print(f"\nâš ï¸ ç”¨æˆ·å–æ¶ˆä¸‹è½½")
            return False
        except Exception as e:
            print(f"âŒ ä¸‹è½½é”™è¯¯: {str(e)}")
            return False
    
    def download_multiple_models(self, model_list):
        """æ‰¹é‡ä¸‹è½½å¤šä¸ªæ¨¡å‹"""
        print(f"\nğŸ“¦ æ‰¹é‡ä¸‹è½½ {len(model_list)} ä¸ªæ¨¡å‹")
        
        # éªŒè¯æ‰€æœ‰æ¨¡å‹
        invalid_models = [m for m in model_list if m not in self.supported_models]
        if invalid_models:
            print(f"âŒ æ— æ•ˆçš„æ¨¡å‹: {invalid_models}")
            return
        
        # è®¡ç®—æ€»ä¼°è®¡å¤§å°
        total_estimate = 0
        for model in model_list:
            size_str = self.supported_models[model]['size_estimate']
            if 'GB' in size_str:
                total_estimate += float(size_str.replace('~', '').replace('GB', '')) * 1024
            else:
                total_estimate += float(size_str.replace('~', '').replace('MB', ''))
        
        print(f"ğŸ’¾ é¢„è®¡æ€»å¤§å°: ~{total_estimate/1024:.1f} GB")
        
        # ç¡®è®¤ä¸‹è½½
        while True:
            choice = input("ç¡®è®¤å¼€å§‹æ‰¹é‡ä¸‹è½½? (y/n): ").lower().strip()
            if choice in ['n', 'no']:
                print("âŒ å–æ¶ˆä¸‹è½½")
                return
            elif choice in ['y', 'yes']:
                break
            else:
                print("è¯·è¾“å…¥ y æˆ– n")
        
        # å¼€å§‹ä¸‹è½½
        successful = []
        failed = []
        
        for i, model_size in enumerate(model_list, 1):
            print(f"\n{'='*20} è¿›åº¦ {i}/{len(model_list)} {'='*20}")
            
            if self.download_single_model(model_size):
                successful.append(model_size)
            else:
                failed.append(model_size)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\n{'='*50}")
        print("ğŸ“Š æ‰¹é‡ä¸‹è½½å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {len(successful)} ä¸ª {successful}")
        if failed:
            print(f"âŒ å¤±è´¥: {len(failed)} ä¸ª {failed}")
        print(f"{'='*50}")
    
    def interactive_mode(self):
        """äº¤äº’å¼æ¨¡å¼"""
        while True:
            print(f"\n{'='*50}")
            print("ğŸ“‹ è¯·é€‰æ‹©æ“ä½œ:")
            print("1. ä¸‹è½½å•ä¸ªæ¨¡å‹")
            print("2. æ‰¹é‡ä¸‹è½½æ¨¡å‹") 
            print("3. æŸ¥çœ‹å·²ä¸‹è½½æ¨¡å‹")
            print("4. ä¸‹è½½æ‰€æœ‰æ¨¡å‹")
            print("0. é€€å‡º")
            print("-" * 50)
            
            try:
                choice = input("è¯·è¾“å…¥é€‰æ‹© (0-4): ").strip()
                
                if choice == '0':
                    print("ğŸ‘‹ å†è§!")
                    break
                elif choice == '1':
                    self._interactive_single_download()
                elif choice == '2':
                    self._interactive_batch_download()
                elif choice == '3':
                    self._show_downloaded_models()
                elif choice == '4':
                    self._download_all_models()
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
                    
            except KeyboardInterrupt:
                print(f"\nğŸ‘‹ å†è§!")
                break
    
    def _interactive_single_download(self):
        """äº¤äº’å¼å•ä¸ªä¸‹è½½"""
        self.show_available_models()
        
        while True:
            model_size = input(f"\nè¯·è¾“å…¥è¦ä¸‹è½½çš„æ¨¡å‹å¤§å° (æˆ– 'back' è¿”å›): ").strip()
            
            if model_size.lower() == 'back':
                return
            elif model_size in self.supported_models:
                self.download_single_model(model_size)
                break
            else:
                print(f"âŒ æ— æ•ˆé€‰æ‹©ï¼Œæ”¯æŒ: {', '.join(self.supported_models.keys())}")
    
    def _interactive_batch_download(self):
        """äº¤äº’å¼æ‰¹é‡ä¸‹è½½"""
        self.show_available_models()
        
        print(f"\nğŸ“¥ æ‰¹é‡ä¸‹è½½æ¨¡å¼")
        print("ğŸ’¡ è¾“å…¥å¤šä¸ªæ¨¡å‹å¤§å°ï¼Œç”¨ç©ºæ ¼æˆ–é€—å·åˆ†éš”")
        print("ğŸ’¡ ä¾‹å¦‚: 124M 355M æˆ– 124M,355M,774M")
        
        while True:
            input_str = input(f"\nè¯·è¾“å…¥æ¨¡å‹åˆ—è¡¨ (æˆ– 'back' è¿”å›): ").strip()
            
            if input_str.lower() == 'back':
                return
            
            # è§£æè¾“å…¥
            if ',' in input_str:
                model_list = [m.strip() for m in input_str.split(',') if m.strip()]
            else:
                model_list = input_str.split()
            
            if not model_list:
                print("âŒ è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæ¨¡å‹å¤§å°")
                continue
            
            # éªŒè¯è¾“å…¥
            invalid = [m for m in model_list if m not in self.supported_models]
            if invalid:
                print(f"âŒ æ— æ•ˆçš„æ¨¡å‹: {invalid}")
                print(f"âœ… æ”¯æŒçš„æ¨¡å‹: {', '.join(self.supported_models.keys())}")
                continue
            
            # å»é‡
            unique_models = list(dict.fromkeys(model_list))
            if len(unique_models) != len(model_list):
                print(f"âš ï¸ å·²å»é™¤é‡å¤æ¨¡å‹")
            
            self.download_multiple_models(unique_models)
            break
    
    def _show_downloaded_models(self):
        """æ˜¾ç¤ºå·²ä¸‹è½½çš„æ¨¡å‹"""
        print(f"\nğŸ“ å·²ä¸‹è½½çš„æ¨¡å‹:")
        print("-" * 60)
        
        downloaded_count = 0
        total_size = 0
        
        for model_size, info in self.supported_models.items():
            if self.check_model_exists(model_size):
                size_mb = self.get_model_disk_size(model_size)
                total_size += size_mb
                downloaded_count += 1
                print(f"âœ… {info['name']:<15} ({model_size}) - {size_mb:.1f} MB")
            else:
                print(f"âŒ {info['name']:<15} ({model_size}) - æœªä¸‹è½½")
        
        print("-" * 60)
        if downloaded_count > 0:
            print(f"ğŸ“Š å·²ä¸‹è½½: {downloaded_count}/{len(self.supported_models)} ä¸ªæ¨¡å‹")
            print(f"ğŸ’¾ æ€»å¤§å°: {total_size:.1f} MB ({total_size/1024:.1f} GB)")
        else:
            print("ğŸ“­ æ²¡æœ‰å·²ä¸‹è½½çš„æ¨¡å‹")
    
    def _download_all_models(self):
        """ä¸‹è½½æ‰€æœ‰æ¨¡å‹"""
        print(f"\nğŸ“¥ ä¸‹è½½æ‰€æœ‰ GPT-2 æ¨¡å‹")
        print(f"âš ï¸ è¿™å°†ä¸‹è½½ {len(self.supported_models)} ä¸ªæ¨¡å‹ï¼Œæ€»è®¡çº¦ 11GB")
        print("âš ï¸ éœ€è¦è¾ƒé•¿æ—¶é—´å’Œå……è¶³çš„ç£ç›˜ç©ºé—´!")
        
        while True:
            choice = input("ç¡®è®¤ä¸‹è½½æ‰€æœ‰æ¨¡å‹? (yes/no): ").lower().strip()
            if choice in ['no', 'n']:
                print("âŒ å–æ¶ˆä¸‹è½½")
                return
            elif choice == 'yes':
                break
            else:
                print("è¯·è¾“å…¥ 'yes' æˆ– 'no'")
        
        all_models = list(self.supported_models.keys())
        self.download_multiple_models(all_models)


def main():
    """ä¸»å‡½æ•°"""
    downloader = GPT2Downloader()
    downloader.show_welcome()
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # å‘½ä»¤è¡Œæ¨¡å¼
        models_to_download = []
        for arg in sys.argv[1:]:
            if ',' in arg:
                models_to_download.extend([m.strip() for m in arg.split(',') if m.strip()])
            else:
                models_to_download.append(arg.strip())
        
        if 'all' in models_to_download:
            models_to_download = list(downloader.supported_models.keys())
        
        # å»é‡å¹¶éªŒè¯
        models_to_download = list(dict.fromkeys(models_to_download))
        invalid_models = [m for m in models_to_download if m not in downloader.supported_models]
        
        if invalid_models:
            print(f"âŒ æ— æ•ˆçš„æ¨¡å‹: {invalid_models}")
            downloader.show_available_models()
            return
        
        if len(models_to_download) == 1:
            downloader.download_single_model(models_to_download[0])
        else:
            downloader.download_multiple_models(models_to_download)
    else:
        # äº¤äº’å¼æ¨¡å¼
        downloader.show_available_models()
        downloader.interactive_mode()


if __name__ == "__main__":
    main()
