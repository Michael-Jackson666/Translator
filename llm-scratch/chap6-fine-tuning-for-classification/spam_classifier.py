import tiktoken 
import torch
import os
import json
import tensorflow as tf
from gpt_download import load_gpt2_params_from_tf_ckpt
from previous_chapters import GPTModel, load_weights_into_gpt

tokenizer = tiktoken.get_encoding("gpt2")


# æ¨¡å‹é…ç½®
BASE_CONFIG = {
    "vocab_size": 50257,     # Vocabulary size
    "context_length": 1024,  # Context length
    "drop_rate": 0.0,        # Dropout rate
    "qkv_bias": True         # Query-key-value bias
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
}

CHOOSE_MODEL = "gpt2-small (124M)"
BASE_CONFIG.update(model_configs[CHOOSE_MODEL])


# ç›´æ¥ä»å·²ä¸‹è½½çš„æ¨¡å‹åŠ è½½æƒé‡
def load_from_existing_files(model_size, models_dir="gpt2"):
    """ç›´æ¥ä»å·²ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶åŠ è½½æƒé‡"""
    model_dir = os.path.join(models_dir, model_size)
    
    # åŠ è½½è®¾ç½®æ–‡ä»¶
    settings_path = os.path.join(model_dir, "hparams.json")
    with open(settings_path, "r", encoding="utf-8") as f:
        settings = json.load(f)
    
    # è·å–checkpointè·¯å¾„å¹¶åŠ è½½å‚æ•°
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    print(f"ğŸ“‚ ä»å·²ä¸‹è½½æ–‡ä»¶åŠ è½½æ¨¡å‹: {tf_ckpt_path}")
    
    # ä½¿ç”¨gpt_download.pyä¸­çš„å‡½æ•°
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)
    
    return settings, params

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
settings, params = load_from_existing_files(model_size=model_size, models_dir="gpt2")

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval()

# æ·»åŠ åˆ†ç±»å¤´
torch.manual_seed(123)
num_classes = 2  # spam or not spam
model.out_head = torch.nn.Linear(
    in_features=BASE_CONFIG["emb_dim"],
    out_features=num_classes
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
print("åŠ è½½é¢„è®­ç»ƒçš„åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨...")
try:
    checkpoint = torch.load("spam_classifier_full_finetune.pth", map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… æˆåŠŸåŠ è½½å…¨å‚æ•°å¾®è°ƒæ¨¡å‹ (å‡†ç¡®ç‡: {checkpoint.get('test_accuracy', 'N/A'):.2%})")
    else:
        model.load_state_dict(checkpoint)
        print("âœ… æˆåŠŸåŠ è½½å…¨å‚æ•°å¾®è°ƒæ¨¡å‹")
except FileNotFoundError:
    try:
        checkpoint = torch.load("spam_classifier_partial_finetune.pth", map_location=device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… æˆåŠŸåŠ è½½éƒ¨åˆ†å¾®è°ƒæ¨¡å‹ (å‡†ç¡®ç‡: {checkpoint.get('test_accuracy', 'N/A'):.2%})")
        else:
            model.load_state_dict(checkpoint)
            print("âœ… æˆåŠŸåŠ è½½éƒ¨åˆ†å¾®è°ƒæ¨¡å‹")
    except FileNotFoundError:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€GPT-2æ¨¡å‹")

# åƒåœ¾é‚®ä»¶åˆ†ç±»å‡½æ•°
def classify_review(text, model, tokenizer, device, max_length=120, pad_token_id=50256):
    model.eval()

    # å‡†å¤‡è¾“å…¥
    input_ids = tokenizer.encode(text)
    
    # æˆªæ–­åºåˆ—ï¼ˆç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦ï¼‰
    input_ids = input_ids[:max_length]
    
    # å¡«å……åºåˆ—
    input_ids += [pad_token_id] * (max_length - len(input_ids))
    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)

    # æ¨¡å‹æ¨ç†
    with torch.no_grad():
        logits = model(input_tensor)[:, -1, :]
    predicted_label = torch.argmax(logits, dim=-1).item()

    return "spam" if predicted_label == 1 else "not spam"

# æµ‹è¯•æ ·ä¾‹
text_1 = "You are a winner you have been specially selected to receive $1000 cash or a $2000 award."
print("ğŸš« åƒåœ¾é‚®ä»¶:", classify_review(text_1, model, tokenizer, device))

text_2 = "Hey, just wanted to check if we're still on for dinner tonight? Let me know!"
print("âœ… æ­£å¸¸é‚®ä»¶:", classify_review(text_2, model, tokenizer, device))