"""
GPT-2 åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ - å…¨å‚æ•°å¾®è°ƒç‰ˆæœ¬
æ”¯æŒå®Œæ•´çš„æ¨¡å‹å¾®è°ƒå’Œé«˜çº§ä¿å­˜åŠŸèƒ½
"""

import tiktoken 
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
import tensorflow as tf
import os
import time
import pickle
import matplotlib.pyplot as plt

# åˆå§‹åŒ–tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):
        self.data = pd.read_csv(csv_file)

        # Pre-tokenize texts
        self.encoded_texts = [
            tokenizer.encode(text) for text in self.data["Text"]
        ]

        if max_length is None:
            self.max_length = self._longest_encoded_length()
        else:
            self.max_length = max_length
            # Truncate sequences if they are longer than max_length
            self.encoded_texts = [
                encoded_text[:self.max_length]
                for encoded_text in self.encoded_texts
            ]

        # Pad sequences to the longest sequence
        self.encoded_texts = [
            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))
            for encoded_text in self.encoded_texts
        ]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self):
        return max(len(encoded_text) for encoded_text in self.encoded_texts)

def load_from_existing_files(model_size, models_dir="gpt2"):
    """ç›´æ¥ä»å·²ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶åŠ è½½æƒé‡"""
    from gpt_download import load_gpt2_params_from_tf_ckpt
    
    model_dir = os.path.join(models_dir, model_size)
    
    # åŠ è½½è®¾ç½®æ–‡ä»¶
    settings_path = os.path.join(model_dir, "hparams.json")
    with open(settings_path, "r", encoding="utf-8") as f:
        settings = json.load(f)
    
    # è·å–checkpointè·¯å¾„å¹¶åŠ è½½å‚æ•°
    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)
    print(f"ğŸ“‚ ä»å·²ä¸‹è½½æ–‡ä»¶åŠ è½½æ¨¡å‹: {tf_ckpt_path}")
    
    # ä½¿ç”¨gpt_download.pyä¸­çš„å‡½æ•°
    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)
    
    return settings, params

def setup_model_and_data():
    """è®¾ç½®æ¨¡å‹å’Œæ•°æ®"""
    print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®...")
    
    # æ¨¡å‹é…ç½®
    CHOOSE_MODEL = "gpt2-small (124M)"
    
    BASE_CONFIG = {
        "vocab_size": 50257,     # Vocabulary size
        "context_length": 1024,  # Context length
        "drop_rate": 0.0,        # Dropout rate
        "qkv_bias": True         # Query-key-value bias
    }

    model_configs = {
        "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
        "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
        "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
        "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
    }

    BASE_CONFIG.update(model_configs[CHOOSE_MODEL])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SpamDataset("train.csv", tokenizer, max_length=None)
    val_dataset = SpamDataset("validation.csv", max_length=train_dataset.max_length, tokenizer=tokenizer)
    test_dataset = SpamDataset("test.csv", max_length=train_dataset.max_length, tokenizer=tokenizer)
    
    # æ£€æŸ¥åºåˆ—é•¿åº¦
    assert train_dataset.max_length <= BASE_CONFIG["context_length"], (
        f"Dataset length {train_dataset.max_length} exceeds model's context "
        f"length {BASE_CONFIG['context_length']}. Reinitialize data sets with "
        f"`max_length={BASE_CONFIG['context_length']}`"
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 8
    torch.manual_seed(123)

    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        drop_last=True,
    )

    val_loader = DataLoader(
        dataset=val_dataset,
        batch_size=batch_size,
        num_workers=0,
        drop_last=False,
    )

    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=batch_size,
        num_workers=0,
        drop_last=False,
    )
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬") 
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {train_dataset.max_length}")
    
    return BASE_CONFIG, CHOOSE_MODEL, train_loader, val_loader, test_loader

def create_model(BASE_CONFIG, CHOOSE_MODEL, fine_tune_all=True):
    """åˆ›å»ºå¹¶é…ç½®æ¨¡å‹"""
    from previous_chapters import GPTModel, load_weights_into_gpt
    
    print("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
    settings, params = load_from_existing_files(model_size=model_size, models_dir="gpt2")

    # åˆ›å»ºæ¨¡å‹
    model = GPTModel(BASE_CONFIG)
    load_weights_into_gpt(model, params)
    model.eval()

    # é…ç½®å¾®è°ƒç­–ç•¥
    if fine_tune_all:
        print("ğŸ”§ ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒç­–ç•¥ï¼šå¾®è°ƒæ‰€æœ‰å±‚çš„å‚æ•°")
        for param in model.parameters():
            param.requires_grad = True
    else:
        print("ğŸ”§ ä½¿ç”¨éƒ¨åˆ†å¾®è°ƒç­–ç•¥ï¼šåªå¾®è°ƒæœ€åä¸€ä¸ªtransformer blockå’Œè¾“å‡ºå±‚")
        for param in model.parameters():
            param.requires_grad = False

    # æ·»åŠ åˆ†ç±»å¤´
    torch.manual_seed(123)
    num_classes = 2  # spam or not spam
    model.out_head = torch.nn.Linear(
        in_features=BASE_CONFIG["emb_dim"],
        out_features=num_classes
    )

    # ç¡®ä¿åˆ†ç±»å¤´å¯è®­ç»ƒ
    for param in model.out_head.parameters():
        param.requires_grad = True
        
    # å¦‚æœæ˜¯éƒ¨åˆ†å¾®è°ƒï¼Œç¡®ä¿æœ€åä¸€å±‚transformerå¯è®­ç»ƒ
    if not fine_tune_all:
        for param in model.trf_blocks[-1].parameters():
            param.requires_grad = True

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {100 * trainable_params / total_params:.2f}%")
    
    return model, fine_tune_all

def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    """è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡"""
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
        
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)

            with torch.no_grad():
                logits = model(input_batch)[:, -1, :]  # Logits of last output token
            predicted_labels = torch.argmax(logits, dim=-1)

            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples

def calc_loss_batch(input_batch, target_batch, model, device):
    """è®¡ç®—å•ä¸ªbatchçš„æŸå¤±"""
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)[:, -1, :]  # Logits of last output token
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss

def calc_loss_loader(data_loader, model, device, num_batches=None):
    """è®¡ç®—æ•°æ®åŠ è½½å™¨çš„å¹³å‡æŸå¤±"""
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
        
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches

def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss

def train_classifier_advanced(model, train_loader, val_loader, optimizer, device, 
                             num_epochs, eval_freq, eval_iter):
    """é«˜çº§è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # åˆå§‹åŒ–è·Ÿè¸ªåˆ—è¡¨
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # ä¸»è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        epoch_start_time = time.time()

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad() # é‡ç½®æ¢¯åº¦
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward() # è®¡ç®—æ¢¯åº¦
            optimizer.step() # æ›´æ–°æƒé‡
            examples_seen += input_batch.shape[0]
            global_step += 1

            # å®šæœŸè¯„ä¼°
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # è®¡ç®—æ¯ä¸ªepochåçš„å‡†ç¡®ç‡
        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)
        
        epoch_time = time.time() - epoch_start_time
        print(f"Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s): "
              f"Train Acc: {train_accuracy*100:.2f}% | Val Acc: {val_accuracy*100:.2f}%")
        
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen

def plot_training_results(epochs_seen, examples_seen, train_values, val_values, label="loss"):
    """ç»˜åˆ¶è®­ç»ƒç»“æœ"""
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æ›²çº¿
    ax1.plot(epochs_seen, train_values, label=f"Training {label}", linewidth=2)
    ax1.plot(epochs_seen, val_values, linestyle="--", label=f"Validation {label}", linewidth=2)
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel(label.capitalize())
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # åˆ›å»ºç¬¬äºŒä¸ªxè½´æ˜¾ç¤ºæ ·æœ¬æ•°
    ax2 = ax1.twiny()
    ax2.plot(examples_seen, train_values, alpha=0)
    ax2.set_xlabel("Examples seen")

    plt.title(f"Training Progress - {label.capitalize()}")
    fig.tight_layout()
    plt.savefig(f"{label}-plot-advanced.pdf", dpi=300, bbox_inches='tight')
    plt.show()

def save_model_advanced(model, config, metrics, fine_tune_all, num_epochs, 
                       training_history, execution_time):
    """é«˜çº§æ¨¡å‹ä¿å­˜åŠŸèƒ½"""
    print("\nğŸ’¾ ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹...")
    
    # ä¿å­˜å®Œæ•´çš„æ¨¡å‹çŠ¶æ€
    model_save_path = f"spam_classifier_{'full' if fine_tune_all else 'partial'}_finetune.pth"
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': config,
        'train_accuracy': metrics['train_accuracy'],
        'val_accuracy': metrics['val_accuracy'], 
        'test_accuracy': metrics['test_accuracy'],
        'fine_tune_all': fine_tune_all,
        'num_epochs': num_epochs,
        'tokenizer_encoding': 'gpt2',
        'training_time_minutes': execution_time,
        'total_params': sum(p.numel() for p in model.parameters()),
        'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)
    }, model_save_path)

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")

    # ä¿å­˜è®­ç»ƒå†å²
    history_save_path = f"training_history_{'full' if fine_tune_all else 'partial'}.pkl"
    
    with open(history_save_path, 'wb') as f:
        pickle.dump(training_history, f)

    print(f"âœ… è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_save_path}")
    
    return model_save_path, history_save_path

def load_fine_tuned_model(model_path, device='cpu'):
    """
    åŠ è½½å¾®è°ƒåçš„åƒåœ¾é‚®ä»¶åˆ†ç±»æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ ('cpu' æˆ– 'cuda')
    
    Returns:
        model: åŠ è½½çš„æ¨¡å‹
        config: æ¨¡å‹é…ç½®
        metrics: æ€§èƒ½æŒ‡æ ‡
    """
    from previous_chapters import GPTModel
    
    checkpoint = torch.load(model_path, map_location=device)
    
    # é‡å»ºæ¨¡å‹
    config = checkpoint['model_config']
    model = GPTModel(config)
    
    # æ·»åŠ åˆ†ç±»å¤´
    model.out_head = torch.nn.Linear(
        in_features=config["emb_dim"],
        out_features=2  # spam or not spam
    )
    
    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    # æ€§èƒ½æŒ‡æ ‡
    metrics = {
        'train_accuracy': checkpoint['train_accuracy'],
        'val_accuracy': checkpoint['val_accuracy'],
        'test_accuracy': checkpoint['test_accuracy'],
        'fine_tune_strategy': 'full' if checkpoint['fine_tune_all'] else 'partial',
        'training_time': checkpoint.get('training_time_minutes', 'N/A'),
        'total_params': checkpoint.get('total_params', 'N/A'),
        'trainable_params': checkpoint.get('trainable_params', 'N/A')
    }
    
    return model, config, metrics

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ GPT-2 åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ - å…¨å‚æ•°å¾®è°ƒç‰ˆæœ¬")
    print("="*60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®æ¨¡å‹å’Œæ•°æ®
    BASE_CONFIG, CHOOSE_MODEL, train_loader, val_loader, test_loader = setup_model_and_data()
    
    # åˆ›å»ºæ¨¡å‹ - è®¾ç½®ä¸ºå…¨å‚æ•°å¾®è°ƒ
    model, fine_tune_all = create_model(BASE_CONFIG, CHOOSE_MODEL, fine_tune_all=True)
    model.to(device)
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 3  # å‡å°‘epochæ•°å› ä¸ºå…¨å‚æ•°å¾®è°ƒè¾ƒæ…¢
    eval_freq = 50
    eval_iter = 5
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    torch.manual_seed(123)

    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_advanced(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=num_epochs, eval_freq=eval_freq, eval_iter=eval_iter,
    )

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {execution_time_minutes:.2f} åˆ†é’Ÿ")

    # ç»˜åˆ¶è®­ç»ƒç»“æœ
    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))
    
    plot_training_results(epochs_tensor, examples_seen_tensor, train_losses, val_losses, "loss")
    
    epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))
    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))
    plot_training_results(epochs_tensor, examples_seen_tensor, train_accs, val_accs, "accuracy")

    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
    train_accuracy = calc_accuracy_loader(train_loader, model, device)
    val_accuracy = calc_accuracy_loader(val_loader, model, device) 
    test_accuracy = calc_accuracy_loader(test_loader, model, device)

    print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:")
    print(f"   è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy*100:.2f}%")
    print(f"   éªŒè¯å‡†ç¡®ç‡: {val_accuracy*100:.2f}%")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy*100:.2f}%")

    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    metrics = {
        'train_accuracy': train_accuracy,
        'val_accuracy': val_accuracy,
        'test_accuracy': test_accuracy
    }
    
    training_history = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accs': train_accs,
        'val_accs': val_accs,
        'examples_seen': examples_seen,
        'execution_time_minutes': execution_time_minutes
    }
    
    model_path, history_path = save_model_advanced(
        model, BASE_CONFIG, metrics, fine_tune_all, 
        num_epochs, training_history, execution_time_minutes
    )
    
    print(f"\nğŸ“– æ¨¡å‹åŠ è½½ç¤ºä¾‹:")
    print(f"model, config, metrics = load_fine_tuned_model('{model_path}')")
    print(f"print(f'Test accuracy: {{metrics[\"test_accuracy\"]*100:.2f}}%')")
    
    print("\nğŸ‰ å¾®è°ƒå®Œæˆ!")

if __name__ == "__main__":
    main()
