"""
ç®€æ´ç‰ˆåƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ - åªåŒ…å«æ ¸å¿ƒåˆ†ç±»åŠŸèƒ½
ç”¨æ³•: python simple_spam_classifier.py "your text here"
"""

import tiktoken
import torch
import sys
import os
from previous_chapters import GPTModel

def load_spam_classifier(model_path=None):
    """åŠ è½½åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨"""
    # è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹
    if model_path is None:
        possible_paths = [
            "spam_classifier_full_finetune.pth",
            "spam_classifier_partial_finetune.pth", 
            "review_classifier.pth"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                model_path = path
                break
        
        if model_path is None:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
    
    # è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)
    
    # é…ç½®
    if 'model_config' in checkpoint:
        config = checkpoint['model_config']
    else:
        config = {
            "vocab_size": 50257, "context_length": 1024, "drop_rate": 0.0,
            "qkv_bias": True, "emb_dim": 768, "n_layers": 12, "n_heads": 12
        }
    
    # åˆ›å»ºæ¨¡å‹
    model = GPTModel(config)
    model.out_head = torch.nn.Linear(config["emb_dim"], 2)
    
    # åŠ è½½æƒé‡
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.to(device)
    model.eval()
    
    return model, device

def classify_spam(text, model=None, device=None):
    """
    åˆ†ç±»æ–‡æœ¬æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶
    
    Args:
        text: è¦åˆ†ç±»çš„æ–‡æœ¬
        model: æ¨¡å‹ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨åŠ è½½ï¼‰
        device: è®¾å¤‡ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨é€‰æ‹©ï¼‰
    
    Returns:
        "spam" æˆ– "not spam"
    """
    # å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹ï¼Œåˆ™åŠ è½½
    if model is None:
        model, device = load_spam_classifier()
    
    # ç¼–ç æ–‡æœ¬
    tokenizer = tiktoken.get_encoding("gpt2")
    input_ids = tokenizer.encode(text)
    
    # å¤„ç†é•¿åº¦
    max_length = 120
    input_ids = input_ids[:max_length]
    input_ids += [50256] * (max_length - len(input_ids))  # padding
    
    # è½¬æ¢ä¸ºtensor
    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)
    
    # é¢„æµ‹
    with torch.no_grad():
        logits = model(input_tensor)[:, -1, :]
        predicted_label = torch.argmax(logits, dim=-1).item()
    
    return "spam" if predicted_label == 1 else "not spam"

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python simple_spam_classifier.py 'ä½ çš„æ–‡æœ¬'")
        print("\nç¤ºä¾‹:")
        print("python simple_spam_classifier.py 'You won $1000! Call now!'")
        print("python simple_spam_classifier.py 'Hi, how are you today?'")
        return
    
    text = " ".join(sys.argv[1:])
    
    try:
        result = classify_spam(text)
        icon = "ğŸš«" if result == "spam" else "âœ…"
        print(f"{icon} {result}: {text}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
