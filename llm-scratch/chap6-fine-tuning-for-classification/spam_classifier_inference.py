"""
GPT-2 åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ - æ¨ç†/é¢„æµ‹ä¸“ç”¨ç‰ˆæœ¬
åªåŒ…å«åŠ è½½æ¨¡å‹å’Œåˆ†ç±»åŠŸèƒ½ï¼Œä¸åŒ…å«è®­ç»ƒä»£ç 
"""

import tiktoken
import torch
import json
import os
from previous_chapters import GPTModel

class SpamClassifier:
    def __init__(self, model_path=None, device=None):
        """
        åˆå§‹åŒ–åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„æ¨¡å‹
            device: è®¾å¤‡ï¼Œé»˜è®¤è‡ªåŠ¨é€‰æ‹©
        """
        self.tokenizer = tiktoken.get_encoding("gpt2")
        self.device = device if device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œå°è¯•æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹
        if model_path is None:
            model_path = self._find_model()
        
        self.model, self.config, self.max_length = self._load_model(model_path)
        print(f"âœ… åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨å·²åŠ è½½ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _find_model(self):
        """è‡ªåŠ¨æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶"""
        possible_paths = [
            "spam_classifier_full_finetune.pth",
            "spam_classifier_partial_finetune.pth", 
            "review_classifier.pth"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                print(f"ğŸ” æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {path}")
                return path
        
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ã€‚è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€å­˜åœ¨: {possible_paths}"
        )
    
    def _load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # è·å–é…ç½®
        if 'model_config' in checkpoint:
            config = checkpoint['model_config']
        else:
            # é»˜è®¤é…ç½®ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            config = {
                "vocab_size": 50257,
                "context_length": 1024,
                "drop_rate": 0.0,
                "qkv_bias": True,
                "emb_dim": 768,
                "n_layers": 12,
                "n_heads": 12
            }
        
        # åˆ›å»ºæ¨¡å‹
        model = GPTModel(config)
        
        # æ·»åŠ åˆ†ç±»å¤´
        model.out_head = torch.nn.Linear(
            in_features=config["emb_dim"],
            out_features=2  # spam or not spam
        )
        
        # åŠ è½½æƒé‡
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(self.device)
        model.eval()
        
        # è®¾ç½®æœ€å¤§é•¿åº¦
        max_length = 120  # é»˜è®¤å€¼ï¼ŒåŸºäºè®­ç»ƒæ•°æ®çš„å…¸å‹é•¿åº¦
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        if 'model_config' in checkpoint:
            print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
            if 'test_accuracy' in checkpoint:
                print(f"   æµ‹è¯•å‡†ç¡®ç‡: {checkpoint['test_accuracy']*100:.2f}%")
            if 'fine_tune_all' in checkpoint:
                strategy = "å…¨å‚æ•°å¾®è°ƒ" if checkpoint['fine_tune_all'] else "éƒ¨åˆ†å¾®è°ƒ"
                print(f"   å¾®è°ƒç­–ç•¥: {strategy}")
        
        return model, config, max_length
    
    def classify_text(self, text, return_confidence=False):
        """
        åˆ†ç±»å•ä¸ªæ–‡æœ¬
        
        Args:
            text: è¦åˆ†ç±»çš„æ–‡æœ¬
            return_confidence: æ˜¯å¦è¿”å›ç½®ä¿¡åº¦
            
        Returns:
            å¦‚æœreturn_confidence=False: "spam" æˆ– "not spam"
            å¦‚æœreturn_confidence=True: (åˆ†ç±»ç»“æœ, ç½®ä¿¡åº¦)
        """
        # ç¼–ç æ–‡æœ¬
        input_ids = self.tokenizer.encode(text)
        
        # æˆªæ–­åºåˆ—
        supported_context_length = self.model.pos_emb.weight.shape[0]
        max_len = min(self.max_length, supported_context_length)
        input_ids = input_ids[:max_len]
        
        # å¡«å……åºåˆ—
        pad_token_id = 50256
        input_ids += [pad_token_id] * (max_len - len(input_ids))
        input_tensor = torch.tensor(input_ids, device=self.device).unsqueeze(0)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            logits = self.model(input_tensor)[:, -1, :]  # æœ€åä¸€ä¸ªtokençš„logits
            probabilities = torch.softmax(logits, dim=-1)
            predicted_label = torch.argmax(logits, dim=-1).item()
            confidence = probabilities[0][predicted_label].item()
        
        result = "spam" if predicted_label == 1 else "not spam"
        
        if return_confidence:
            return result, confidence
        else:
            return result
    
    def classify_batch(self, texts, return_confidence=False):
        """
        æ‰¹é‡åˆ†ç±»æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            return_confidence: æ˜¯å¦è¿”å›ç½®ä¿¡åº¦
            
        Returns:
            åˆ†ç±»ç»“æœåˆ—è¡¨
        """
        results = []
        for text in texts:
            result = self.classify_text(text, return_confidence)
            results.append(result)
        return results
    
    def classify_with_details(self, text):
        """
        è¯¦ç»†åˆ†ç±»ç»“æœï¼ŒåŒ…å«æ‰€æœ‰ä¿¡æ¯
        
        Args:
            text: è¦åˆ†ç±»çš„æ–‡æœ¬
            
        Returns:
            å­—å…¸åŒ…å«åˆ†ç±»ç»“æœã€ç½®ä¿¡åº¦å’Œæ¦‚ç‡
        """
        # ç¼–ç æ–‡æœ¬
        input_ids = self.tokenizer.encode(text)
        
        # æˆªæ–­åºåˆ—
        supported_context_length = self.model.pos_emb.weight.shape[0]
        max_len = min(self.max_length, supported_context_length)
        input_ids = input_ids[:max_len]
        
        # å¡«å……åºåˆ—
        pad_token_id = 50256
        input_ids += [pad_token_id] * (max_len - len(input_ids))
        input_tensor = torch.tensor(input_ids, device=self.device).unsqueeze(0)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            logits = self.model(input_tensor)[:, -1, :]
            probabilities = torch.softmax(logits, dim=-1)
            predicted_label = torch.argmax(logits, dim=-1).item()
        
        result = "spam" if predicted_label == 1 else "not spam"
        confidence = probabilities[0][predicted_label].item()
        
        return {
            "text": text,
            "prediction": result,
            "confidence": confidence,
            "probabilities": {
                "not_spam": probabilities[0][0].item(),
                "spam": probabilities[0][1].item()
            },
            "text_length": len(text),
            "token_count": len(input_ids)
        }

def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    print("ğŸ¯ GPT-2 åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ - æ¨ç†ç‰ˆæœ¬")
    print("="*50)
    
    # åˆå§‹åŒ–åˆ†ç±»å™¨
    try:
        classifier = SpamClassifier()
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹æ–‡ä»¶")
        return
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "You are a winner you have been specially selected to receive $1000 cash or a $2000 award.",
        "Hey, just wanted to check if we're still on for dinner tonight? Let me know!",
        "URGENT! You have won Â£2000! Call now to claim your prize!",
        "Hi mom, can you pick me up from school at 3pm today?",
        "Free phone! Text WIN to 12345 to get your free smartphone now!",
        "The meeting has been rescheduled to tomorrow at 2pm. Please confirm your attendance."
    ]
    
    print("ğŸ” å•ä¸ªæ–‡æœ¬åˆ†ç±»ç¤ºä¾‹:")
    print("-" * 50)
    
    for i, text in enumerate(test_texts[:3], 1):
        result, confidence = classifier.classify_text(text, return_confidence=True)
        print(f"{i}. æ–‡æœ¬: {text[:50]}...")
        print(f"   ç»“æœ: {result} (ç½®ä¿¡åº¦: {confidence:.3f})")
        print()
    
    print("ğŸ“Š è¯¦ç»†åˆ†ç±»ç¤ºä¾‹:")
    print("-" * 50)
    
    sample_text = test_texts[0]
    details = classifier.classify_with_details(sample_text)
    
    print(f"æ–‡æœ¬: {details['text'][:60]}...")
    print(f"é¢„æµ‹: {details['prediction']}")
    print(f"ç½®ä¿¡åº¦: {details['confidence']:.3f}")
    print(f"æ¦‚ç‡åˆ†å¸ƒ:")
    print(f"  æ­£å¸¸é‚®ä»¶: {details['probabilities']['not_spam']:.3f}")
    print(f"  åƒåœ¾é‚®ä»¶: {details['probabilities']['spam']:.3f}")
    print(f"æ–‡æœ¬é•¿åº¦: {details['text_length']} å­—ç¬¦")
    print(f"Tokenæ•°é‡: {details['token_count']}")
    
    print("\nğŸš€ æ‰¹é‡åˆ†ç±»ç¤ºä¾‹:")
    print("-" * 50)
    
    batch_results = classifier.classify_batch(test_texts)
    for text, result in zip(test_texts, batch_results):
        status = "ğŸš«" if result == "spam" else "âœ…"
        print(f"{status} {result}: {text[:60]}...")

if __name__ == "__main__":
    main()
